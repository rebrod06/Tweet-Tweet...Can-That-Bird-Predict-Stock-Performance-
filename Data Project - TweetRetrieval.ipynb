{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Retrieval\n",
    "\n",
    "In this notebook we have setup all of the necessary imports, keys and objects to gather our tweets. The Python package, Twython, was used to retrieve the tweets from Twitter. All of the data has already been gathered and saved as CSV files. \n",
    "\n",
    "***NOTE:*** *No cell blocks have been commented out. Running this notebook will trigger the tweet retrieval process, which takes 45 minutes to complete.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from twython import Twython\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#autocomplete\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "#create twython object\n",
    "twitter = Twython()\n",
    "\n",
    "#input key and secret code\n",
    "APP_KEY = '8W2Xn6qLQyrj1MFoBHXtXpquL'\n",
    "APP_SECRET = 'DYGu67FqOBepDoem92tKsoVrpCVI8yGQ0OjwRDqd23mbjMNcWL'\n",
    "twitter = Twython(APP_KEY, APP_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for gathering tweets required a crash course in Twitter's API. In order to search for and gather tweets based on a specific date and company name, we created a generator (Twython's cursor() method) to traverse and return specific results. After our first pull, we quickly realized the majority of tweets were retweets or company responses to individual tweets. We decided to omit both of these from our results. After reviewing the results from several pulls, we narrowed our search parameters to the following:\n",
    "- Company name (Twitter handle)\n",
    "- Date\n",
    "- English tweets\n",
    "- Original tweets (no retweets)\n",
    "- Extended tweets (full text)\n",
    "\n",
    "Since Twitter's maximum search request is 450 every 15 minutes, we decided to gather 1800 tweets per company per day, for a total search time of 3 hours each day. Unfortunately, Twython was very tempermental and most pulls resulted in connection errors, especially during peak business hours (9 AM - 5 PM). As such, we opted to manually run the code below for each pull rather than all at once for all 4 companies (see note in code).\n",
    "\n",
    "Each Twitter object contains *numerous* attributes and sub-objects. In order to make our analysis more interesting, we decided to gather the following attributes for 3 sub-objects:\n",
    "\n",
    "**Tweet**\n",
    "- Tweet text\n",
    "- Tweet ID\n",
    "- Date\n",
    "- Number of times retweeted\n",
    "- Number of times favorited\n",
    "- Popularity\n",
    "\n",
    "**User**\n",
    "- User ID\n",
    "- Number of followers\n",
    "- Number of friends\n",
    "- Number of tweets generated\n",
    "\n",
    "**Entity** (data in tweet)\n",
    "- Number of links\n",
    "- Number of hashtags \n",
    "- URL\n",
    "\n",
    "The following code generates three datasets containing tweet, user and entity data for each company's tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## GRAB 1800 COMPANY TWEETS\n",
    "# params to change in query: @company name, date\n",
    "#company_tags = ['@drpepper', '@MonsterEnergy', '@CocaCola','@pepsi']\n",
    "\n",
    "company_tag = '@drpepper'\n",
    "\n",
    "#initialize lists to hold df data\n",
    "userdf_lists = []\n",
    "tweetdf_lists = []\n",
    "entitydf_lists = []\n",
    "\n",
    "#initialize variables for loop\n",
    "num_tweets = 450 #corresponds to count parameter in .cursor() function; how many tweets we get at a time\n",
    "counter = 0\n",
    "stopper = num_tweets\n",
    "last_id = 0\n",
    "query = company_tag+' since:2018-04-16 -filter:nativeretweets' #all company tweets since date, exclude retweets\n",
    "\n",
    "#get first tweet id for max_id\n",
    "result = twitter.search(q=query, count=1, lang='en', tweet_mode=\"extended\")\n",
    "for tweet in result['statuses']:\n",
    "    last_id = tweet['id']\n",
    "    \n",
    "# #manually assign last_id\n",
    "# last_id = 983199594661150720\n",
    "    \n",
    "\n",
    "#NOTE: we could have looped through list of company handles to retrieve tweets for \n",
    "#all companies at once, but connection error were inevitable and would stop the entire process\n",
    "\n",
    "#grab 450 tweets every 15 minutes until we've gotten 1800 (this will take 45 mins)\n",
    "while counter < 1800: \n",
    "    results = twitter.cursor(twitter.search, q=query, lang='en', count=num_tweets, tweet_mode=\"extended\", max_id=last_id)\n",
    "    for tweet in results:\n",
    "        #collect user data\n",
    "        userid = tweet['user']['id']\n",
    "        followers = tweet['user']['followers_count']\n",
    "        friends = tweet['user']['friends_count']\n",
    "        statuses = tweet['user']['statuses_count']\n",
    "        if not tweet['user']['url']:\n",
    "            url = 0\n",
    "        else:\n",
    "            url = 1\n",
    "        #create list of user info to add to user df\n",
    "        user_list = [userid, followers, friends, statuses, url, company_tag] \n",
    "\n",
    "        #collect tweet data\n",
    "        last_id = tweet['id']\n",
    "        text = tweet['full_text']\n",
    "        retweets = tweet['retweet_count']\n",
    "        favorited = tweet['favorite_count']\n",
    "        date = tweet['created_at']\n",
    "        popularity = tweet['metadata']['result_type']\n",
    "        #create list of tweet info to add to tweet df\n",
    "        tweet_list = [last_id, text, retweets, favorited, date, popularity, userid, company_tag]\n",
    "\n",
    "        #collect entity data\n",
    "        if not tweet['entities']['hashtags']:\n",
    "            hashtags = 0\n",
    "        else:\n",
    "            hashtags = len(tweet['entities']['hashtags'])\n",
    "\n",
    "        if not tweet['entities']['urls']:\n",
    "            ent_url = 0\n",
    "        else:\n",
    "            ent_url = 1\n",
    "        #create list of entity info to add to entity df\n",
    "        entity_list = [hashtags, ent_url, last_id, company_tag]\n",
    "\n",
    "        #add data to lists (these will go in csv file when we're done collecting)\n",
    "        userdf_lists.append(user_list)\n",
    "        tweetdf_lists.append(tweet_list)\n",
    "        entitydf_lists.append(entity_list)\n",
    "\n",
    "        counter += 1\n",
    "        if counter >= stopper:\n",
    "            break\n",
    "\n",
    "    #update stopper\n",
    "    stopper += num_tweets\n",
    "\n",
    "    #print to indicate pause\n",
    "    print(counter, \" tweets\", 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "\n",
    "    #pause 15 mins before getting more tweets\n",
    "    if counter != 1800:\n",
    "        print(\"PAUSE\")\n",
    "        time.sleep(900)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(\"----DONE----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below generate three file names and converts a dataset into a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##FUNCTIONS TO GENERATE CSV\n",
    "\n",
    "#function that generates three csv filenames for each dataset\n",
    "#params: name of company(string), date (string, formated \"month_day\")\n",
    "#return: list of 3 string filenames\n",
    "def make_filenames(name, month_day):\n",
    "    user_fname = month_day+\"_user_data_\"+name+\".csv\"\n",
    "    tweet_fname = month_day+\"_tweet_data_\"+name+\".csv\"\n",
    "    entity_fname = month_day+\"_entity_data_\"+name+\".csv\"\n",
    "    return [user_fname, tweet_fname, entity_fname]\n",
    "\n",
    "#function that generates three csv files for each filename\n",
    "#params: list of 3 filenames, list of datasets (userdf_lists, tweetdf_lists, entitydf_lists)\n",
    "def make_csv_files(filenames, datasets):\n",
    "    for name, item in zip(filenames, datasets):\n",
    "        with open(name, \"w\", encoding='utf-8-sig') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate three CSV files from the results of one pull. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EXPORT TO CSV\n",
    "\n",
    "#date for files\n",
    "date_match = re.search(r\"0[3-4]-[0-9]+\", query)\n",
    "#list of datasets\n",
    "datasets = [userdf_lists, tweetdf_lists, entitydf_lists]\n",
    "\n",
    "#generate csv files for company based on query\n",
    "if (re.search(r\"^@drpepper\", query)):\n",
    "    make_csv_files(make_filenames(\"drp\", date_match.group()), datasets)\n",
    "elif (re.search(r\"^@MonsterEnergy\", query)):\n",
    "    make_csv_files(make_filenames(\"monst\", date_match.group()), datasets)\n",
    "elif (re.search(r\"^@CocaCola\", query)):\n",
    "    make_csv_files(make_filenames(\"coke\", date_match.group()), datasets)   \n",
    "else:\n",
    "    make_csv_files(make_filenames(\"pepsi\", date_match.group()), datasets)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
